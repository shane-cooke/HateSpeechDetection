{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0485e2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Hateful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>Damn I thought they had strict gun laws in Ger...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>I dont care about what it stands for or anythi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>It's not a group it's an idea lol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>So it's not just America!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>The dog is a spectacular dancer considering he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>This is what Kike shilling looks like. Ryan ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>Not bait, they're right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>I like this one a lot.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>Kikes making money off heroin what’s new</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>Desecrate men by making them gay/trannies, and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Platform                                            Comment  Hateful\n",
       "0      Reddit  Damn I thought they had strict gun laws in Ger...        0\n",
       "1      Reddit  I dont care about what it stands for or anythi...        0\n",
       "2      Reddit                  It's not a group it's an idea lol        0\n",
       "3      Reddit                          So it's not just America!        0\n",
       "4      Reddit  The dog is a spectacular dancer considering he...        0\n",
       "...       ...                                                ...      ...\n",
       "2995    4Chan  This is what Kike shilling looks like. Ryan ha...        1\n",
       "2996    4Chan                            Not bait, they're right        0\n",
       "2997    4Chan                             I like this one a lot.        0\n",
       "2998    4Chan           Kikes making money off heroin what’s new        1\n",
       "2999    4Chan  Desecrate men by making them gay/trannies, and...        1\n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "df = pd.read_csv('/Users/shanecooke/Desktop/Official_GitLab/CompleteData.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5876c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Hateful</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>Damn I thought they had strict gun laws in Ger...</td>\n",
       "      <td>0</td>\n",
       "      <td>damn think strict gun law germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>I dont care about what it stands for or anythi...</td>\n",
       "      <td>0</td>\n",
       "      <td>dont care stand anything connect like shield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>It's not a group it's an idea lol</td>\n",
       "      <td>0</td>\n",
       "      <td>group idea lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>So it's not just America!</td>\n",
       "      <td>0</td>\n",
       "      <td>america</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>The dog is a spectacular dancer considering he...</td>\n",
       "      <td>0</td>\n",
       "      <td>dog spectacular dancer consider two leave foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>This is what Kike shilling looks like. Ryan ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>kike shill look like ryan do redpill people fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>Not bait, they're right</td>\n",
       "      <td>0</td>\n",
       "      <td>bait right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>I like this one a lot.</td>\n",
       "      <td>0</td>\n",
       "      <td>like one lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>Kikes making money off heroin what’s new</td>\n",
       "      <td>1</td>\n",
       "      <td>kike make money heroin whats new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>Desecrate men by making them gay/trannies, and...</td>\n",
       "      <td>1</td>\n",
       "      <td>desecrate men make gay trannies woman make abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Platform                                            Comment  Hateful  \\\n",
       "0      Reddit  Damn I thought they had strict gun laws in Ger...        0   \n",
       "1      Reddit  I dont care about what it stands for or anythi...        0   \n",
       "2      Reddit                  It's not a group it's an idea lol        0   \n",
       "3      Reddit                          So it's not just America!        0   \n",
       "4      Reddit  The dog is a spectacular dancer considering he...        0   \n",
       "...       ...                                                ...      ...   \n",
       "2995    4Chan  This is what Kike shilling looks like. Ryan ha...        1   \n",
       "2996    4Chan                            Not bait, they're right        0   \n",
       "2997    4Chan                             I like this one a lot.        0   \n",
       "2998    4Chan           Kikes making money off heroin what’s new        1   \n",
       "2999    4Chan  Desecrate men by making them gay/trannies, and...        1   \n",
       "\n",
       "                                             clean_text  \n",
       "0                     damn think strict gun law germany  \n",
       "1          dont care stand anything connect like shield  \n",
       "2                                        group idea lol  \n",
       "3                                               america  \n",
       "4        dog spectacular dancer consider two leave foot  \n",
       "...                                                 ...  \n",
       "2995  kike shill look like ryan do redpill people fa...  \n",
       "2996                                         bait right  \n",
       "2997                                       like one lot  \n",
       "2998                   kike make money heroin whats new  \n",
       "2999  desecrate men make gay trannies woman make abo...  \n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preProcessText(text):\n",
    "    text = text.lower() \n",
    "    text = text.strip()  \n",
    "    text = re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    "def stopwordRemoval(string):\n",
    "    stop = [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(stop)\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "def tagMapping(tag):\n",
    "    if tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatization(string):\n",
    "    words = nltk.pos_tag(word_tokenize(string))\n",
    "    temp = [wl.lemmatize(tag[0], tagMapping(tag[1])) for idx, tag in enumerate(words)]\n",
    "    return \" \".join(temp)\n",
    "\n",
    "def finalCleaning(string):\n",
    "    return lemmatization(stopwordRemoval(preProcessText(string)))\n",
    "\n",
    "df['clean_text'] = df['Comment'].apply(lambda x: finalCleaning(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11caa3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Hateful</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tfidf1</th>\n",
       "      <th>tfidf2</th>\n",
       "      <th>Doc2Vec</th>\n",
       "      <th>Doc2Vec1</th>\n",
       "      <th>Doc2Vec2</th>\n",
       "      <th>hashing</th>\n",
       "      <th>hashing1</th>\n",
       "      <th>hashing2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>Damn I thought they had strict gun laws in Ger...</td>\n",
       "      <td>0</td>\n",
       "      <td>damn think strict gun law germany</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0032718100119382143, 0.004408253822475672, ...</td>\n",
       "      <td>[-0.08356256037950516, 0.17799748480319977, 0....</td>\n",
       "      <td>[-0.08634454011917114, 0.18881037831306458, 0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.4082482904638631, 0.0, 0.0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>I dont care about what it stands for or anythi...</td>\n",
       "      <td>0</td>\n",
       "      <td>dont care stand anything connect like shield</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0018799236277118325, 0.005465839523822069, ...</td>\n",
       "      <td>[-0.048040103167295456, 0.10511787235736847, 0...</td>\n",
       "      <td>[0.023344872519373894, 0.02822037972509861, -0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>It's not a group it's an idea lol</td>\n",
       "      <td>0</td>\n",
       "      <td>group idea lol</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.00534618878737092, 0.008697368204593658, 0....</td>\n",
       "      <td>[-0.02423957549035549, 0.0676441639661789, 0.0...</td>\n",
       "      <td>[-0.03381773456931114, 0.10322832316160202, 0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>So it's not just America!</td>\n",
       "      <td>0</td>\n",
       "      <td>america</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.004059546627104282, -0.0004826425283681601...</td>\n",
       "      <td>[-0.02149725891649723, 0.03351963311433792, 0....</td>\n",
       "      <td>[-0.06831774860620499, 0.12614895403385162, 0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>The dog is a spectacular dancer considering he...</td>\n",
       "      <td>0</td>\n",
       "      <td>dog spectacular dancer consider two leave foot</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.0009879578137770295, 0.010099951177835464,...</td>\n",
       "      <td>[-0.08533000200986862, 0.17648042738437653, 0....</td>\n",
       "      <td>[-0.11566188931465149, 0.2142035961151123, 0.1...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>This is what Kike shilling looks like. Ryan ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>kike shill look like ryan do redpill people fa...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[2.1984729755786248e-05, -0.000203559786314144...</td>\n",
       "      <td>[-0.06838290393352509, 0.13812002539634705, 0....</td>\n",
       "      <td>[-0.0753791481256485, 0.2228599190711975, 0.11...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>Not bait, they're right</td>\n",
       "      <td>0</td>\n",
       "      <td>bait right</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0048239585012197495, -0.0038504567928612232...</td>\n",
       "      <td>[-0.022948168218135834, 0.050892189145088196, ...</td>\n",
       "      <td>[-0.0495951883494854, 0.10468236356973648, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>I like this one a lot.</td>\n",
       "      <td>0</td>\n",
       "      <td>like one lot</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.0073617687448859215, -0.000852906494401395...</td>\n",
       "      <td>[-0.002204886404797435, -0.010703779757022858,...</td>\n",
       "      <td>[-0.023860253393650055, 0.08026639372110367, 0...</td>\n",
       "      <td>[0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>Kikes making money off heroin what’s new</td>\n",
       "      <td>1</td>\n",
       "      <td>kike make money heroin whats new</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.009042023681104183, -0.0013995234621688724...</td>\n",
       "      <td>[-0.047436658293008804, 0.07360194623470306, 0...</td>\n",
       "      <td>[-0.10244839638471603, 0.16428011655807495, 0....</td>\n",
       "      <td>[0.0, 0.0, -0.4082482904638631, 0.408248290463...</td>\n",
       "      <td>[0.0, 0.0, -0.4082482904638631, 0.408248290463...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>4Chan</td>\n",
       "      <td>Desecrate men by making them gay/trannies, and...</td>\n",
       "      <td>1</td>\n",
       "      <td>desecrate men make gay trannies woman make abo...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.00631176820024848, 0.0078119877725839615, ...</td>\n",
       "      <td>[-0.14290904998779297, 0.27673229575157166, 0....</td>\n",
       "      <td>[-0.1777052879333496, 0.3495290279388428, 0.24...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Platform                                            Comment  Hateful  \\\n",
       "0      Reddit  Damn I thought they had strict gun laws in Ger...        0   \n",
       "1      Reddit  I dont care about what it stands for or anythi...        0   \n",
       "2      Reddit                  It's not a group it's an idea lol        0   \n",
       "3      Reddit                          So it's not just America!        0   \n",
       "4      Reddit  The dog is a spectacular dancer considering he...        0   \n",
       "...       ...                                                ...      ...   \n",
       "2995    4Chan  This is what Kike shilling looks like. Ryan ha...        1   \n",
       "2996    4Chan                            Not bait, they're right        0   \n",
       "2997    4Chan                             I like this one a lot.        0   \n",
       "2998    4Chan           Kikes making money off heroin what’s new        1   \n",
       "2999    4Chan  Desecrate men by making them gay/trannies, and...        1   \n",
       "\n",
       "                                             clean_text  \\\n",
       "0                     damn think strict gun law germany   \n",
       "1          dont care stand anything connect like shield   \n",
       "2                                        group idea lol   \n",
       "3                                               america   \n",
       "4        dog spectacular dancer consider two leave foot   \n",
       "...                                                 ...   \n",
       "2995  kike shill look like ryan do redpill people fa...   \n",
       "2996                                         bait right   \n",
       "2997                                       like one lot   \n",
       "2998                   kike make money heroin whats new   \n",
       "2999  desecrate men make gay trannies woman make abo...   \n",
       "\n",
       "                                                  tfidf  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                 tfidf1  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                 tfidf2  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                Doc2Vec  \\\n",
       "0     [0.0032718100119382143, 0.004408253822475672, ...   \n",
       "1     [0.0018799236277118325, 0.005465839523822069, ...   \n",
       "2     [0.00534618878737092, 0.008697368204593658, 0....   \n",
       "3     [-0.004059546627104282, -0.0004826425283681601...   \n",
       "4     [-0.0009879578137770295, 0.010099951177835464,...   \n",
       "...                                                 ...   \n",
       "2995  [2.1984729755786248e-05, -0.000203559786314144...   \n",
       "2996  [0.0048239585012197495, -0.0038504567928612232...   \n",
       "2997  [-0.0073617687448859215, -0.000852906494401395...   \n",
       "2998  [-0.009042023681104183, -0.0013995234621688724...   \n",
       "2999  [-0.00631176820024848, 0.0078119877725839615, ...   \n",
       "\n",
       "                                               Doc2Vec1  \\\n",
       "0     [-0.08356256037950516, 0.17799748480319977, 0....   \n",
       "1     [-0.048040103167295456, 0.10511787235736847, 0...   \n",
       "2     [-0.02423957549035549, 0.0676441639661789, 0.0...   \n",
       "3     [-0.02149725891649723, 0.03351963311433792, 0....   \n",
       "4     [-0.08533000200986862, 0.17648042738437653, 0....   \n",
       "...                                                 ...   \n",
       "2995  [-0.06838290393352509, 0.13812002539634705, 0....   \n",
       "2996  [-0.022948168218135834, 0.050892189145088196, ...   \n",
       "2997  [-0.002204886404797435, -0.010703779757022858,...   \n",
       "2998  [-0.047436658293008804, 0.07360194623470306, 0...   \n",
       "2999  [-0.14290904998779297, 0.27673229575157166, 0....   \n",
       "\n",
       "                                               Doc2Vec2  \\\n",
       "0     [-0.08634454011917114, 0.18881037831306458, 0....   \n",
       "1     [0.023344872519373894, 0.02822037972509861, -0...   \n",
       "2     [-0.03381773456931114, 0.10322832316160202, 0....   \n",
       "3     [-0.06831774860620499, 0.12614895403385162, 0....   \n",
       "4     [-0.11566188931465149, 0.2142035961151123, 0.1...   \n",
       "...                                                 ...   \n",
       "2995  [-0.0753791481256485, 0.2228599190711975, 0.11...   \n",
       "2996  [-0.0495951883494854, 0.10468236356973648, 0.0...   \n",
       "2997  [-0.023860253393650055, 0.08026639372110367, 0...   \n",
       "2998  [-0.10244839638471603, 0.16428011655807495, 0....   \n",
       "2999  [-0.1777052879333496, 0.3495290279388428, 0.24...   \n",
       "\n",
       "                                                hashing  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2997  [0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2998  [0.0, 0.0, -0.4082482904638631, 0.408248290463...   \n",
       "2999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                               hashing1  \\\n",
       "0     [0.0, 0.0, 0.0, -0.4082482904638631, 0.0, 0.0,...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2997  [0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2998  [0.0, 0.0, -0.4082482904638631, 0.408248290463...   \n",
       "2999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                               hashing2  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "2995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2999  [0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[3000 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import texthero as hero\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "df['tfidf'] = (hero.tfidf(df['clean_text'], max_features=1000))\n",
    "\n",
    "df['tfidf1'] = (hero.tfidf(df['clean_text'], max_features=2000))\n",
    "\n",
    "df['tfidf2'] = (hero.tfidf(df['clean_text'], max_features=3000))\n",
    "\n",
    "card_docs = [TaggedDocument(doc.split(' '), [i]) for i, doc in enumerate(df.clean_text)]\n",
    "model = Doc2Vec(vector_size=64, window=2, min_count=1, workers=8, epochs = 10)\n",
    "model.build_vocab(card_docs)\n",
    "model.train(card_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "temp = [model.infer_vector((df['clean_text'][i].split(' '))) for i in range(0,len(df['clean_text']))]\n",
    "dtv = np.array(temp).tolist()\n",
    "df['Doc2Vec'] = dtv\n",
    "\n",
    "card_docs = [TaggedDocument(doc.split(' '), [i]) for i, doc in enumerate(df.clean_text)]\n",
    "model = Doc2Vec(vector_size=64, window=2, min_count=1, workers=8, epochs = 20)\n",
    "model.build_vocab(card_docs)\n",
    "model.train(card_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "temp = [model.infer_vector((df['clean_text'][i].split(' '))) for i in range(0,len(df['clean_text']))]\n",
    "dtv = np.array(temp).tolist()\n",
    "df['Doc2Vec1'] = dtv\n",
    "\n",
    "card_docs = [TaggedDocument(doc.split(' '), [i]) for i, doc in enumerate(df.clean_text)]\n",
    "model = Doc2Vec(vector_size=64, window=2, min_count=1, workers=8, epochs = 40)\n",
    "model.build_vocab(card_docs)\n",
    "model.train(card_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "temp = [model.infer_vector((df['clean_text'][i].split(' '))) for i in range(0,len(df['clean_text']))]\n",
    "dtv = np.array(temp).tolist()\n",
    "df['Doc2Vec2'] = dtv\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=100)\n",
    "hashed = vectorizer.transform(df['clean_text']).toarray()\n",
    "hashList = np.array(hashed).tolist()\n",
    "df['hashing'] = hashList\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=300)\n",
    "hashed = vectorizer.transform(df['clean_text']).toarray()\n",
    "hashList = np.array(hashed).tolist()\n",
    "df['hashing1'] = hashList\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=500)\n",
    "hashed = vectorizer.transform(df['clean_text']).toarray()\n",
    "hashList = np.array(hashed).tolist()\n",
    "df['hashing2'] = hashList\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b5495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanecooke/miniforge3/envs/virt/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:\n",
      "\n",
      "Max_features = 1000\n",
      "-------------------\n",
      "Accuracy:  0.946667\n",
      "Precision:  0.96\n",
      "Recall:  0.78\n",
      "F1 Score:  0.86\n",
      "\n",
      "\n",
      "Max_features = 2000\n",
      "-------------------\n",
      "Accuracy:  0.956667\n",
      "Precision:  0.94\n",
      "Recall:  0.83\n",
      "F1 Score:  0.89\n",
      "\n",
      "\n",
      "Max_features = 3000\n",
      "-------------------\n",
      "Accuracy:  0.937778\n",
      "Precision:  0.97\n",
      "Recall:  0.74\n",
      "F1 Score:  0.84\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from statistics import mean\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "print(\"TFIDF:\\n\")\n",
    "params = [\"Max_features = 1000\", \"Max_features = 2000\", \"Max_features = 3000\"]\n",
    "\n",
    "def wordEmbeddingOptimisationTFIDF():\n",
    "    for i in range(0, 3):\n",
    "        X, y = df[df.columns[i+4]].tolist(), df.Hateful\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        \n",
    "        classifier = RandomForestClassifier(n_estimators=1000)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        f1score = f1_score(y_test, y_pred)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(params[i])\n",
    "        print(\"-------------------\")\n",
    "        print(\"Accuracy: \", round(accuracy_score(y_test, y_pred), 6))\n",
    "        print(\"Precision: \", round(precision_score(y_test, y_pred), 2))\n",
    "        print(\"Recall: \", round(recall_score(y_test, y_pred),2))\n",
    "        print(\"F1 Score: \", round(f1_score(y_test, y_pred), 2))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "wordEmbeddingOptimisationTFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b095340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec:\n",
      "\n",
      "Epochs = 10\n",
      "-------------------\n",
      "Accuracy:  0.808889\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 Score:  0.0\n",
      "\n",
      "\n",
      "Epochs = 20\n",
      "-------------------\n",
      "Accuracy:  0.792222\n",
      "Precision:  0.46\n",
      "Recall:  0.06\n",
      "F1 Score:  0.11\n",
      "\n",
      "\n",
      "Epochs = 40\n",
      "-------------------\n",
      "Accuracy:  0.897778\n",
      "Precision:  0.91\n",
      "Recall:  0.55\n",
      "F1 Score:  0.68\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Doc2Vec:\\n\")\n",
    "params = [\"Epochs = 10\", \"Epochs = 20\", \"Epochs = 40\"]\n",
    "\n",
    "def wordEmbeddingOptimisationDoc2Vec():\n",
    "    for i in range(0, 3):\n",
    "        X, y = df[df.columns[i+7]].tolist(), df.Hateful\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        \n",
    "        classifier = LinearDiscriminantAnalysis(solver='eigen')\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        f1score = f1_score(y_test, y_pred)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(params[i])\n",
    "        print(\"-------------------\")\n",
    "        print(\"Accuracy: \", round(accuracy_score(y_test, y_pred), 6))\n",
    "        print(\"Precision: \", round(precision_score(y_test, y_pred), 2))\n",
    "        print(\"Recall: \", round(recall_score(y_test, y_pred),2))\n",
    "        print(\"F1 Score: \", round(f1_score(y_test, y_pred), 2))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "wordEmbeddingOptimisationDoc2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ef867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing:\n",
      "\n",
      "N_features = 100\n",
      "-------------------\n",
      "Accuracy:  0.86\n",
      "Precision:  0.62\n",
      "Recall:  0.53\n",
      "F1 Score:  0.57\n",
      "\n",
      "\n",
      "N_features = 300\n",
      "-------------------\n",
      "Accuracy:  0.895556\n",
      "Precision:  0.81\n",
      "Recall:  0.59\n",
      "F1 Score:  0.68\n",
      "\n",
      "\n",
      "N_features = 500\n",
      "-------------------\n",
      "Accuracy:  0.898889\n",
      "Precision:  0.89\n",
      "Recall:  0.6\n",
      "F1 Score:  0.72\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Hashing:\\n\")\n",
    "params = [\"N_features = 100\", \"N_features = 300\", \"N_features = 500\"]\n",
    "\n",
    "def wordEmbeddingOptimisationHashing():\n",
    "    for i in range(0, 3):\n",
    "        X, y = df[df.columns[i+10]].tolist(), df.Hateful\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        \n",
    "        classifier = XGBClassifier(verbosity = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        f1score = f1_score(y_test, y_pred)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(params[i])\n",
    "        print(\"-------------------\")\n",
    "        print(\"Accuracy: \", round(accuracy_score(y_test, y_pred), 6))\n",
    "        print(\"Precision: \", round(precision_score(y_test, y_pred), 2))\n",
    "        print(\"Recall: \", round(recall_score(y_test, y_pred),2))\n",
    "        print(\"F1 Score: \", round(f1_score(y_test, y_pred), 2))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "wordEmbeddingOptimisationHashing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756dccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
